---
permalink: /articles/
title: "Publications"
excerpt: "Publications"
author_profile: true
redirect_from: 
  - "/wordpress/"
  - "/wordpress/index.html"
---

{% include base_path %}

(*) denotes equal contribution, (**) denotes equal advising


## Journal Submissions

**[JS.2]** [Convergence Rates for Softmax Gating Mixture of Experts](https://arxiv.org/abs/2503.03213) *Under review*. <br/>
<b>Huy Nguyen</b>, Nhat Ho\*\*, Alessandro Rinaldo\*\*, 

**[JS.1]** [On Expert Estimation in Hierarchical Mixture of Experts: Beyond Softmax Gating Functions](https://arxiv.org/pdf/2410.02935) *Under review*. <br/>
<b>Huy Nguyen\*</b>, Xing Han\*, Carl William Harris, Suchi Saria\*\*, Nhat Ho\*\*

## Conference Submissions

**[CS.6]** [Sigmoid Self-Attention is Better than Softmax Self-Attention: A Mixture-of-Experts Perspective
](https://www.arxiv.org/abs/2502.00281). *Under review* <br/>
<b>Huy Nguyen\*</b>, Fanqi Yan\*, Pedram Akbarian, Nhat Ho\*\*, Alessandro Rinaldo\*\*

**[CS.5]** [RepLoRA: Reparameterizing Low-rank Adaptation via the Perspective of Mixture of Experts](https://arxiv.org/abs/2502.03044). *Under review* <br/>
Tuan Truong\*, Chau Nguyen\*, <b>Huy Nguyen\*</b>, Minh Le, Trung Le, Nhat Ho

**[CS.4]** [Adaptive Prompt: Unlocking the Power of Visual Prompt Tuning](https://arxiv.org/abs/2501.18936). *Under review* <br/>
Minh Le\*, Anh Nguyen\*, <b>Huy Nguyen</b>, Chau Nguyen, Nhat Ho

**[CS.3]** [On Zero-Initialized Attention: Optimal Prompt and Gating Factor Estimation](https://arxiv.org/abs/2502.03029). *Under review* <br/>
Nghiem T. Diep\*, <b>Huy Nguyen\*</b>, Chau Nguyen\*, Minh Le, Duy M. H. Nguyen, Daniel Sonntag, Mathias Niepert, Nhat Ho

**[CS.2]** [Quadratic Gating Functions in Mixture of Experts: A Statistical Insight](https://arxiv.org/pdf/2410.11222.pdf). *Under review*. <br/>
Pedram Akbarian\*, <b>Huy Nguyen\*</b>, Xing Han\*, Nhat Ho

**[CS.1]** [CompeteSMoE - Effective Training of Sparse Mixture of Experts via Competition](https://arxiv.org/pdf/2402.02526.pdf). *Under review*. <br/>
Quang Pham, Giang Do, <b>Huy Nguyen</b>, TrungTin Nguyen, Chenghao Liu, Mina Sartipi, Binh T. Nguyen, Savitha Ramasamy, Xiaoli Li, Steven Hoi, Nhat Ho

## Conference Publications

**[C.19]** [Statistical Advantages of Perturbing Cosine Router in Sparse Mixture of Experts](https://arxiv.org/pdf/2405.14131.pdf). *Proceedings of the ICLR, 2025*. <br/>
<b>Huy Nguyen</b>, Pedram Akbarian\*, Trang Pham\*, Trang Nguyen\*, Shujian Zhang, Nhat Ho

**[C.18]** [Revisiting Prefix-tuning: Statistical Benefits of Reparameterization among Prompts](https://arxiv.org/pdf/2410.02200). *Proceedings of the ICLR, 2025*. <br/>
Minh Le\*, Chau Nguyen\*, <b>Huy Nguyen\*</b>, Quyen Tran, Trung Le, Nhat Ho

**[C.17]** [Understanding Expert Structures on Minimax Parameter Estimation in Contaminated Mixture of Experts](https://arxiv.org/pdf/2410.12258.pdf). *In AISTATS, 2025*. <br/>
Fanqi Yan\*, <b>Huy Nguyen\*</b>, Dung Le\*, Pedram Akbarian, Nhat Ho

**[C.16]** [Sigmoid Gating is More Sample Efficient than Softmax Gating in Mixture of Experts](https://arxiv.org/pdf/2405.13997.pdf). *Advances in NeurIPS, 2024*. <br/>
<b>Huy Nguyen</b>, Nhat Ho\*\*, Alessandro Rinaldo\*\*

**[C.15]** [FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion](https://arxiv.org/pdf/2402.03226.pdf). *Advances in NeurIPS, 2024*. <br/>
Xing Han, <b>Huy Nguyen\*</b>, Carl Harris\*, Nhat Ho\*\*, Suchi Saria\*\*

**[C.14]** [Mixture of Experts Meets Prompt-Based Continual Learning](https://arxiv.org/pdf/2405.14124.pdf). *Advances in NeurIPS, 2024*. <br/>
Minh Le, An Nguyen\*, <b>Huy Nguyen\*</b>, Trang Nguyen\*, Trang Pham\*, Linh Van Ngo, Nhat Ho

**[C.13]** [On Least Square Estimation in Softmax Gating Mixture of Experts](https://arxiv.org/pdf/2402.02952.pdf). *Proceedings of the ICML, 2024*. <br/>
<b>Huy Nguyen</b>, Nhat Ho\*\*, Alessandro Rinaldo\*\*

**[C.12]** [Is Temperature Sample Efficient for Softmax Gaussian Mixture of Experts?](https://arxiv.org/pdf/2401.13875.pdf). *Proceedings of the ICML, 2024*. <br/>
<b>Huy Nguyen</b>, Pedram Akbarian, Nhat Ho

**[C.11]** [A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts](https://arxiv.org/pdf/2310.14188.pdf). *Proceedings of the ICML, 2024*. <br/>
<b>Huy Nguyen</b>, Pedram Akbarian, TrungTin Nguyen, Nhat Ho

**[C.10]** [Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts](https://arxiv.org/pdf/2309.13850.pdf). *Proceedings of the ICLR, 2024*. <br/>
<b>Huy Nguyen</b>, Pedram Akbarian, Fanqi Yan, Nhat Ho

**[C.9]** [Towards Convergence Rates for Parameter Estimation in Gaussian-gated Mixture of Experts](https://arxiv.org/pdf/2305.07572.pdf). *In AISTATS, 2024*. <br/>
<b>Huy Nguyen\*</b>, TrungTin Nguyen\*, Khai Nguyen, Nhat Ho

**[C.8]** [On Parameter Estimation in Deviated Gaussian Mixture of Experts](https://arxiv.org/pdf/2402.05220.pdf). *In AISTATS, 2024*. <br/>
<b>Huy Nguyen</b>, Khai Nguyen, Nhat Ho

**[C.7]** [Demystifying Softmax Gating Function in Gaussian Mixture of Experts](https://arxiv.org/pdf/2305.03288.pdf). *Advances in NeurIPS, 2023* <span style="color:red"> **(Spotlight)** </span>. <br/>
<b>Huy Nguyen</b>, TrungTin Nguyen, Nhat Ho

**[C.6]** [Minimax Optimal Rate for Parameter Estimation in Multivariate Deviated Models](https://arxiv.org/pdf/2301.11808.pdf). *Advances in NeurIPS, 2023*. <br/>
Dat Do\*, <b>Huy Nguyen\*</b>, Khai Nguyen, Nhat Ho

**[C.5]** [Fast Approximation of the Generalized Sliced-Wasserstein Distance](https://openreview.net/forum?id=u3JeFO8G8s). *In ICASSP, 2024*. <br/>
Dung Le\*, <b>Huy Nguyen\*</b>, Khai Nguyen\*, Trang Nguyen\*, Nhat Ho

**[C.4]** [Hierarchical Sliced Wasserstein Distance](https://openreview.net/pdf?id=CUOaVn6mYEj). *Proceedings of the ICLR, 2023*. <br/>
Khai Nguyen, Tongzheng Ren, <b>Huy Nguyen</b>, Litu Rout, Tan Nguyen, Nhat Ho

**[C.3]** [Entropic Gromov-Wasserstein between Gaussian Distributions](https://proceedings.mlr.press/v162/le22a.html). *Proceedings of the ICML, 2022*. <br/>
<b>Huy Nguyen\*</b>, Khang Le\*, Dung Le\*, Dat Do, Tung Pham, Nhat Ho

**[C.2]** [On Multimarginal Partial Optimal Transport: Equivalent Forms and Computational Complexity](https://proceedings.mlr.press/v151/le22a.html). *Proceedings of the AISTATS, 2022*. <br/>
<b>Huy Nguyen\*</b>, Khang Le\*, Khai Nguyen, Tung Pham, Nhat Ho

**[C.1]** [On Robust Optimal Transport: Computational Complexity and Barycenter Computation](https://proceedings.neurips.cc/paper/2021/hash/b80ba73857eed2a36dc7640e2310055a-Abstract.html). *Advances in NeurIPS, 2021*. <br/>
<b>Huy Nguyen\*</b>, Khang Le\*, Quang Minh Nguyen, Tung Pham, Hung Bui, Nhat Ho

## Journal Publications

**[J.1]** [EPEM: Efficient Parameter Estimation for Multiple Class Monotone Missing Data](https://www.sciencedirect.com/science/article/abs/pii/S0020025521002346). *Information Sciences, Volume 567, August 2021, Pages 1-22*. <br/>
Thu Nguyen, Duy H. M. Nguyen, <b>Huy Nguyen</b>, Binh T. Nguyen, Bruce A. Wade.


